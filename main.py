"""
In this project, we will analyze key metrics of IronHack Payments using the data provided. 
Our goal is to extract insights into user behavior and the performance of their financial services. 
To achieve this, we will follow these steps:

1. Loading the data: Import datasets into Python for analysis.
2. Understanding the data: Explore the structure, types, and contents of the data to identify key features.
3. Data Quality Assessment: Detect and document missing values, inconsistencies, and duplicates.
4. Cleaning the data: Address issues identified in the quality assessment to ensure data reliability.
5. Exploratory Data Analysis (EDA) and Visualization: Analyze patterns and trends, using visualizations to uncover insights.
6. Conclusions and Hypotheses: Summarize findings, propose actionable recommendations, and generate hypotheses for further analysis.
"""

# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# 1. Load Data
def load_data():
    cash_data = pd.read_csv("project_dataset/cash_request_data_analyst.csv")
    fees_data = pd.read_csv("project_dataset/fees_data_analyst.csv")
    return cash_data, fees_data

# 2. Understanding the Data
def explore_data(cash_data, fees_data):
    print("Cash Request Data Info:")
    print(cash_data.info(), "\n" + "-"*100 + "\n")
    print("Example of cash_request_df:\n", cash_data.head(2), "\n" + "-"*100 + "\n")

    print("Fees Data Info:")
    print(fees_data.info(), "\n" + "-"*100 + "\n")
    print("Example of fees_df:\n", fees_data.head(2), "\n" + "-"*100 + "\n")

    print("Cash Request Summary Statistics:")
    print(cash_data.describe(), "\n" + "-"*100 + "\n")

    print("Fees Data Summary Statistics:")
    print(fees_data.describe(), "\n" + "-"*100 + "\n")

# 3. Data Quality Assessment
def assess_data_quality(cash_data, fees_data):
    print("Missing Values in Cash Requests:")
    print(cash_data.isnull().sum(), "\n" + "-"*100 + "\n")

    print("Missing Values in Fees Data:")
    print(fees_data.isnull().sum(), "\n" + "-"*100 + "\n")

    print("Duplicated Rows in Cash Requests:", cash_data.duplicated().sum())
    print("Duplicated Rows in Fees Data:", fees_data.duplicated().sum(), "\n" + "-"*100 + "\n")


    # Add check for spaces

# 4. Cleaning the data
def cleaning_data(cash_data, fees_data):
    # Eliminate unnecessary columns
    cash_data = cash_data.drop(columns=["updated_at","moderated_at","reimbursement_date","cash_request_received_date","money_back_date","transfer_type","send_at"])
    fees_data = fees_data.drop(columns=["type","category","reason","updated_at","paid_at","from_date","to_date","charge_moment"])

    # Ensure datetime format for date columns
    cash_data['created_at'] = pd.to_datetime(cash_data['created_at']).dt.strftime('%Y-%m-%d %H:%M:%S')
    fees_data['created_at'] = pd.to_datetime(fees_data['created_at']).dt.strftime('%Y-%m-%d %H:%M:%S')

    cash_data['reco_creation'] = pd.to_datetime(cash_data['reco_creation']).dt.strftime('%Y-%m-%d %H:%M:%S')
    cash_data['reco_last_update'] = pd.to_datetime(cash_data['reco_last_update']).dt.strftime('%Y-%m-%d %H:%M:%S')

    cash_data['created_at'] = pd.to_datetime(cash_data['created_at'])
    fees_data['created_at'] = pd.to_datetime(fees_data['created_at'])

    cash_data['reco_creation'] = pd.to_datetime(cash_data['reco_creation'])
    cash_data['reco_last_update'] = pd.to_datetime(cash_data['reco_last_update'])


    # Replace missing user_id with deleted_account_id if available
    cash_data['user_id'] = cash_data.apply(
        lambda row: row['user_id'] if pd.notnull(row['user_id']) else row['deleted_account_id'],
        axis=1
    )

    # Drop rows where both user_id and deleted_account_id are missing
    cash_data = cash_data.dropna(subset=['user_id'])

    # Define cohort (month the request)
    cash_data['cohort'] = cash_data['created_at'].dt.to_period('M')
    fees_data['cohort'] = fees_data['created_at'].dt.to_period('M')

    # Create column called "resolution_time" calculating the difference between each date
    cash_data['resolution_time'] = (cash_data['reco_last_update']-cash_data['reco_creation']).dt.days
    print("Cash and Fees data cleaned correctly")

    return cash_data, fees_data


# 5. Exploratory Data Analysis (EDA) 
def exploratory_analisis(cash_data, fees_data):

    # 1. Frequency of Service Usage
    frequency_service_usage(cash_data)

    # 2. Incident Rate
    #incident_rate(cash_data)

    # 3. Revenue Generated by the Cohort
    #revenue_generated(fees_data)

    # 4. New Relevant Metric (Efficiency of the support service)
    #efficiency_support_service(cash_data)

    return print("Finished EDA correctly")

    
def frequency_service_usage(cash_data):
    # Count how many rows are in each cohort and convert the series to dataframe
    cohort_frequency_service = (cash_data[cash_data['status']!='cancelled'].cohort.value_counts()).to_frame().sort_values(by=['cohort'])

    # Giving names to column
    cohort_frequency_service.columns = ['total_requests']

    # Show result
    # Crear el gráfico de barras usando seaborn
    plt.figure(figsize=(12, 6))
    sns.barplot(x='cohort', y='total_requests', data=cohort_frequency_service, palette='Blues_d')

    # Etiquetas y título
    plt.xlabel('Cohort')
    plt.ylabel('Total Cash Requests')
    plt.title('Frequency of Service Usage')
    plt.savefig('results/frequency_service_usage.png', bbox_inches='tight')
    
    return print("Frequency Service Usage img generated correctly at results/frequency_service_usage.png")


def incident_rate(cash_data):

    # Total incidents per cohort (only where recovery_status is not null)
    total_incidents = cash_data[cash_data['recovery_status'].notnull()].groupby('cohort')['id'].count()

    # Total requests per cohort (all requests, regardless of incidents)
    total_requests = cash_data.groupby('cohort')['id'].count()

    # Ensure all cohorts are present in total_incidents (even those with 0 incidents)
    total_incidents = total_incidents.reindex(total_requests.index, fill_value=0)

    # Calculate the incident rate as the percentage of incidents per cohort
    incident_rate = (total_incidents / total_requests * 100)

    # Create a DataFrame for results
    cohort_incident_rate = pd.concat([total_requests, total_incidents, incident_rate], axis=1)
    cohort_incident_rate.columns = ['total_requests', 'total_incidents', 'relative_frequency']

    plt.figure(figsize=(15, 6))

    # Crear el gráfico de líneas para la tasa de incidentes
    plt.plot(cohort_incident_rate.index.astype(str), cohort_incident_rate['relative_frequency'], marker='o', color='b')

    # Añadir etiquetas y título
    plt.xlabel('Cohort')
    plt.ylabel('Incident Rate (%)')
    plt.title('Evolution of incident rate per cohort')
    plt.xticks(ha='center')

    # Ajustar y guardar el gráfico
    plt.savefig('results/incident_rate.png', bbox_inches='tight')

    return print("Incident Rate img generated correctly at results/incident_rate.png")


def revenue_generated(fees_data):
    # Group by cohort and sum the total_amount of each cohort, only where status = accepted
    cohort_total_amount = fees_data[fees_data['status'] == 'accepted'].groupby('cohort')['total_amount'].sum()

    # Transform to dataframe
    cohort_total_amount = cohort_total_amount.reset_index()

    # Give names to columns
    cohort_total_amount.columns = ['cohort', 'total_amount']

    # Set cohort column as index
    cohort_total_amount = cohort_total_amount.set_index('cohort')

    # Crear el gráfico de barras usando seaborn
    plt.figure(figsize=(10, 6))
    sns.barplot(x='cohort', y='total_amount', data=cohort_total_amount, palette='Greens_r')

    # Etiquetas y título
    plt.xlabel('Cohort')
    plt.ylabel('Revenue generated')
    plt.title('Total revenue generated')
    plt.savefig('results/total_revenue_generated.png', bbox_inches='tight')
    
    return print("Total revenue img generated correctly at results/total_revenue_generated.png")


def efficiency_support_service(cash_data):
    """The idea of this metric is value how fast is the support service of IronHack Payments
        in resolve an incident, for this we will calcule the mean time between the date of 
        incident creation and his closure date"""
    
    mean_resolution_time = (cash_data[cash_data['recovery_status']=='completed'].groupby('cohort')['resolution_time'].mean()).reset_index()

    # Crear el gráfico de barras usando seaborn
    plt.figure(figsize=(12, 6))
    sns.barplot(x='cohort', y='resolution_time', data=mean_resolution_time, palette="flare")

    # Etiquetas y título
    plt.xlabel('Cohort')
    plt.ylabel('Resolution time (days)')
    plt.title('Mean time for resolve incident')
    plt.savefig('results/mean_resolution_time.png', bbox_inches='tight')
    
    return print("Support service efficiency img generated correctly at results/mean_resolution_time.png")

# Main Script
if __name__ == "__main__":
    # Step 1: Load the data
    cash_request_df, fees_df = load_data()

    # Step 2: Understand the data
    #explore_data(cash_request_df, fees_df)

    # Step 3: Assess data quality
    assess_data_quality(cash_request_df, fees_df)

    # Step 4: Cleaning the data
    cash_request_df, fees_df = cleaning_data(cash_request_df, fees_df)

    cash_request_df.to_csv("project_dataset/cleaned_cash_request_data_analyst.csv")
    fees_df.to_csv("project_dataset/cleaned_fees_data_analyst.csv")

    # Step 5: Exploratory Data Analysis (EDA)
    exploratory_analisis(cash_request_df, fees_df)
